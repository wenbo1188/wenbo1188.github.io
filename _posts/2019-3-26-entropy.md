---
layout: post
title: NLP学习(8)之Entropy
date: 2019-03-26
categories: NLP
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# Information Entropy #

[Information Entropy](https://www.youtube.com/watch?v=R4OlXb9aTvQ)

$$H(x) = -\sum_iP(x_i)log(P(x_i))$$  
信息熵定义为信息量的期望，信息量为$$-log(P(x_i))$$

# Cross-entropy and KL-divergence #

[cross-entropy and kl-divergence](https://www.youtube.com/watch?v=ErfnhcEV1O8)

在信息论中，基于相同事件测度的两个概率分布*p*和*q*的交叉熵是指，当基于一个“非自然”（相对于“真实”分布p而言）的概率分布q进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。

基于概率分布p和q的交叉熵定义为：

$$H(p, q)=E_{p}[-log q]=H(p)+D_{KL}(p||q)$$  

其中H(p)是p的熵，$$D_{KL}(p\|\|q)$$是从p到q的KL散度(也被称为p相对于q的相对熵)。

对于离散分布p和q，这意味着：  
$$H(p,q)=-\sum_xp(x)log q(x)$$  
对于连续分布也是类似的。我们假设p和q在测度r上是绝对连续的(通常r是Lebesgue measure on a Borel $$\sigma$$-algebra)。设P和Q分别为和q在测度r上概率密度函数。则

$$-\int_{X}P(x)log Q(x)dr(x) = E_{p}[-log Q]$$