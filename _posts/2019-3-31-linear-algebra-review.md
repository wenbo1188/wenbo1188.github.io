---
layout: post
title: NLP学习(12)之线性代数复习
date: 2019-03-31
categories: NLP
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 基础知识 #

矩阵Hadamard乘积：元素对应乘积，记为$$A \bigodot B$$  
两个向量点积满足交换律：$$x^Ty = y^Tx$$  
列向量线性相关的方阵被称为奇异阵(不可逆阵)  

## 范数 ##

机器学习中，经常使用被称为范数(norm)的函数来衡量向量的大小。形式上$$L^P$$范数定义如下：  
$$\parallel x \parallel_P = (\sum_i|x_i|^P)^{\frac{1}{P}}$$，其中$$P \in R, P\geq1$$  
范数是将向量映射到非负值的函数，直观上讲，向量$$x$$的范数衡量原点到点$$x$$的距离。  
范数具有以下性质:  

1. $$f(x) = 0 \Rightarrow x = 0$$
2. $$f(x+y) \leq f(x) + f(y)$$
3. $$\forall \alpha \in R, f(\alpha x) = |\alpha|f(x)$$

当$$P=2$$时，$$L^2$$称为欧几里得范数，表示原点到点$$x$$的欧几里得距离，在机器学习中，$$L^2$$范数出现十分频繁，常简化表示为$$\parallel x \parallel$$，省略下标2。  
平方$$L^2$$范数也经常用来衡量向量的大小，可以简单通过点积$$x^Tx$$计算。  
平方$$L^2$$范数优点：  

1. 表达和计算更简便
2. 对$$x$$中每个元素的导数只取决于对应元素，而$$L^2$$范数对每个元素倒数却于整个向量有关

缺点：在原点附近增长缓慢  
在某些机器学习应用中，区分恰好是零和非零但很小是很重要的，这时我们常使用各位置斜率相同，而且简洁的$$L^1$$范数。$$L^1$$范数也经常作为表示非零元素数目的替代函数。  
另外一个经常使用的范数是$$L^{\infty}$$范数，也称为最大范数，这个范数表示向量中具有最大幅值元素的绝对值：$$\parallel x \parallel_{\infty} = max_i|x_i|$$  
衡量矩阵大小，常用Frobenius范数：  
$$\parallel A \parallel_F = \sqrt{\sum_{i,j} {A_{i,j}^2}}$$  

# 特殊的矩阵和向量 #

1. 对角矩阵  
特点：
    - 矩阵乘法很方便  
    - 求逆很方便  
$$diag(v) x = v \bigodot x$$  
2. 对称矩阵：$$A = A^T$$
3. 单位向量  
标准正交：在$$R^n$$中，至多有n个范数非零向量正交，如这些向量不仅相互正交而且范数都为1，那么他们标准正交。
4. 正交矩阵：  
$$ A^TA = AA^T = I$$，其中$$I$$为单位阵  
$$A^{-1} = A^T$$  
求逆代价小。
正交矩阵的行向量不仅正交，而且标准正交。  

# 特征分解 #

$$ Av = \lambda v$$  
$$\lambda$$称为特征值，$$v$$为对应的特征向量，若$$v$$是$$A$$的特征向量，那么任何缩放后的$$sv(s \neq 0)$$也是$$A$$的特征向量，此外$$v$$和$$sv$$有相同的特征值，故通常只考虑单位特征向量，设$$A$$有$$n$$个线性无关特征向量$$\{v^{(1)},...,v^{(n)}\}$$，对应特征值为$$\{\lambda_1,...,\lambda_n\}$$，记$$V = [v^{(1)},...,v^{(n)}]$$，类似地，我们也可以将特征值连接成一个向量$$\lambda = [\lambda_1,...,\lambda_n]^T$$  
$$A$$可以分解为：  
$$A = V diag(\lambda) V^{-1}$$  
每个实对称矩阵都可以分解成实特征向量和实特征值。  
$$A = Q\Lambda Q^T$$  
其中$$Q$$是$$A$$的特征向量组成的正交矩阵，\Lambda是对角矩阵，因为$$Q$$是正交矩阵，可以将$$A$$看作沿方向$$v^{(i)}$$延展$$\lambda$$倍的空间。  

# 奇异值分解(SVD) #

$$A = UDV^T$$  
假设$$A$$是一个mxn阵，$$D$$是一个mxn阵，$$v$$是一个nxn阵，其中$$U$$, $$V$$都是正交阵，而$$D$$是对角阵($$D$$不一定是方阵)。  
对角阵$$D$$的对角线上元素被称为矩阵$$A$$的奇异值，矩阵$$U$$的列向量被称为左奇异向量，$$V$$的列向量被称为右奇异向量，事实上$$A$$的左奇异向量是$$AA^T$$的特征向量，A的右奇异向量是$$A^TA$$的特征向量，$$A$$的非零奇异值是$$A^TA$$特征值的平方根，同时也是$$AA^T$$的平方根。  

# Moore-Penrose伪逆 #

对非方阵，逆矩阵没有定义。  
假设$$Ax = y$$等式两边左乘左逆$$B$$后，我们得到$$x = By$$取决于问题的形式，可能无法设计一个唯一的映射将$$A$$映射到$$B$$。  
Moore-Penrose伪逆：  
矩阵A的伪逆：$$A^+ = lim_{\alpha \rightarrow 0}(A^TA + \alpha I)^{-1}A^T$$  
实际中用下面公式：  
$$A^+ = VD^+U^T$$  
其中矩阵$$U$$, $$D$$, $$V$$是$$A$$奇异值分解后的矩阵，对角阵$$D$$的伪逆$$D^+$$是其非零元素取倒数之后再转置得到的。  

# 主成分分析(PCA) #

输入：n维样本集$$X = (x_1,x_2,...,x_m)$$，要降维到n维
输出：降维后的样本集$$Y$$  
1. 对所有样本进行中心化$$x_i = x_i - \frac{1}{m}\sum_{j=1}^mx_j$$
2. 计算样本的协方差矩阵$$C = \frac{1}{m}XX^T$$  
3. 求出协方差矩阵的特征值及对应的特征向量  
4. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前$$k$$行组成矩阵$$P$$  
5. $$Y = PX$$即为降维到k维后的数据